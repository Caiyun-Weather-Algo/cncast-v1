## ------- models for training and inference ------- ##
model:
  name: swin_transformer_3d_slidepatch
  era5_vae: era5_autoencoder
  cmpa_vae: cmpa_autoencoder
  lpips_discriminator: None
  swin_transformer_3d_slidepatch:
    in_chans: 5
    patch_size: [2,4,4]
    embed_dim: 256
    window_size: [2,7,7]
    depths: [3, 9, 3]
    num_heads: [4, 8, 16]
    add_boundary: True
    use_checkpoint: False
  dit:
    input_size: [256,256]
    cond_size: [256,256]
    img_size: [256,256]
    in_channels: 1
    out_channels: 1
    cond_channels: 69
  dit_cmpa:
    input_size: [32,32]
    cond_size: [32,32]
    img_size: [32,32]
    in_channels: 1
    out_channels: 1
    cond_channels: 256
  swinir:
    upscale: 4
    img_size: [256, 256]
    in_chans: 1
    out_chans: 1
    window_size: 4
    patch_size: 4
  era5_tp_autoencoder:  
    embed_dim: 16
    latent_shape: [16, 32, 32]
    scaling_factor: 0.68027   ## std of era5 latent is about 1.47
    ckpt_path: /home/lianghongli/weather-us-central1/lianghl/precip_diff/ldm_autoencoder_exp3/models/ldm_autoencoder_consolidated_best.pth.tar
    lossconfig:
      target: src.models.contperceptual.LPIPSWithDiscriminator
      params:
        disc_start: 20001
        kl_weight: 0.000001
        disc_weight: 0.5
    ddconfig:
      target: era5
      double_z: True
      z_channels: 16
      resolution: 256
      in_channels: 1
      out_ch: 1
      ch: 16
      ch_mult: [ 1, 2, 4, 8]  # num_down = len(ch_mult)-1
      num_res_blocks: 2
      attn_resolutions: [16, 8]
      dropout: 0.0
  era5_autoencoder:  
    embed_dim: 256
    latent_shape: [256, 32, 32]
    scaling_factor: 0.56497   ## std of era5 latent is about 1.77
    ckpt_path: /home/lianghongli/weather-us-central1/lianghl/precip_diff/ldm_autoencoder_exp4/models/ldm_autoencoder_consolidated_best.pth.tar
    lossconfig:
      target: src.models.contperceptual.LPIPSWithDiscriminator
      params:
        disc_start: 20001
        kl_weight: 0.000001
        disc_weight: 0.5
    ddconfig:
      target: era5
      double_z: True
      z_channels: 128
      resolution: 256
      in_channels: 69
      out_ch: 69
      ch: 128
      ch_mult: [ 1, 2, 4, 8]  # num_down = len(ch_mult)-1
      num_res_blocks: 2
      attn_resolutions: [16, 8]
      dropout: 0.0
  cmpa_autoencoder:
    embed_dim: 16
    latent_shape: [16, 32, 32]
    scaling_factor: 0.76923   ## std of cmpa latent is about 1.3
    # ckpt_path: /home/lianghongli/weather-us-central1/hess/autoencoder_kl_gan_expcmpa/models/autoencoder_kl_gan_best.pth.tar
    ckpt_path: /home/lianghongli/weather-us-central1/lianghl/precip_diff/ldm_autoencoder_exp5/models/ldm_autoencoder_consolidated_best.pth.tar
    lossconfig:
      target: src.models.contperceptual.LPIPSWithDiscriminator
      params:
        disc_start: 50001
        kl_weight: 0.000001
        disc_weight: 0.5
    ddconfig:
      target: cmpa
      double_z: True
      z_channels: 16
      resolution: 900
      in_channels: 1
      out_ch: 1
      ch: 16
      # ch_mult: [ 1, 2, 4, 4]  # num_down = len(ch_mult)-1
      ch_mult: [ 1, 2, 4, 8,16,16]  # num_down = len(ch_mult)-1
      num_res_blocks: 2
      attn_resolutions: [16, 8]
      dropout: 0.0
  lpipsdiscriminator:
    disc_start: 20001
    kl_weight: 0.00001
    disc_weight: 0.5
    perceptual_weight: 1
    disc_in_channels: 70
  pretrain: None
  finetune: None

## ------- hyper parameters for training ------- ##
hps:
  batch_size: 64
  EPOCH: 100
  lr: 3e-4
  optimizer: AdamW
  weight_decay: 0.0001
    #scheduler: reducelronplateau
  scheduler: cosineannealingwarmrestart
  warmup_lr: False
  grad_accum: 1
  loss: mse
  verbose_step: 5000
  
## ------ configs for loading different data for different model training and inference ------- ##
dataload:
  hist4in: 0
  fcst4out: 1
  sample_interval: 1
  use_dem: False
  norm_method: meanstd
  start_time: "1980010100" ## 2020010100 for valid, 2021010100 for test
  end_time: "2021122700"
  target: era5
  cut_era5: False ## True for ERA5 VAE training and cmpa diagnosis
  cmpa_frame: 1
  resize_data: False
  with_era5_tp: False


exp: 1
fcst_tp: False
loss_method: mse
loss_weighting: False
autoregress_step: 1
base_path: /home/lianghongli/bucket/cncast_v1
ckpt_prefix: /home/lianghongli/caiyun-reg-fcst/himawari-caiyun/swin_transformer_3d


buckets:
  project: colorful-aia
  bucket_base: weather-us-central1
  bucket_ifs: ecmwf-open-data
  bucket_era5: era5_china_src

## ------ input variables for training and inference ------- ##
input:
  surface:
  - 2mt
  - mslp
  - 10m_u_component_of_wind
  - 10m_v_component_of_wind
  high:
  - geopotential
  - temperature
  - specific_humidity
  - u_component_of_wind
  - v_component_of_wind
  levels: 
  - 1000
  - 950
  - 850
  - 700
  - 600
  - 500
  - 450
  - 400
  - 300
  - 250
  - 200
  - 150
  - 100

output:
  surface:
  - 2mt
  - mslp
  - 10m_u_component_of_wind
  - 10m_v_component_of_wind
  high:
  - geopotential
  - temperature
  - specific_humidity
  - u_component_of_wind
  - v_component_of_wind